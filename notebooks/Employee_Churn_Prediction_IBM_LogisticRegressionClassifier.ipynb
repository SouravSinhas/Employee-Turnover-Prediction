{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import chardet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import heapq\n",
    "import re\n",
    "import plotnine\n",
    "import copy\n",
    "import pydot\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols, logit \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing processed data into notebook\n",
    "\n",
    "sourcepath = 'D:\\Work\\Research\\Dropbox\\Machine Learning\\Side Projects\\Employee Churn IBM\\employee_churn_prediction_ibm'\n",
    "datapath = os.path.join(sourcepath, 'data','processed','IBM_HR_Data_Cleaned.csv')\n",
    "\n",
    "IBMData = pd.read_csv(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant variables\n",
    "\n",
    "IBMData.drop(['EmployeeNumber'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the types of some variables \n",
    "\n",
    "numvars = ['Age','DailyRate','DistanceFromHome','HourlyRate','MonthlyIncome','MonthlyRate','NumCompaniesWorked', \n",
    "          'PercentSalaryHike','PerformanceRating','TotalWorkingYears','TrainingTimesLastYear','YearsAtCompany',\n",
    "          'YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\n",
    "newnumvars = ['Education','EnvironmentSatisfaction','JobInvolvement','JobLevel','JobSatisfaction','PerformanceRating',\n",
    "              'RelationshipSatisfaction','StockOptionLevel','WorkLifeBalance']\n",
    "catvars = [item for item in IBMData.columns.to_list() if item not in numvars and item not in newnumvars and item != 'Attrition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IBMData[numvars] = IBMData[numvars].apply(pd.to_numeric, axis = 1, downcast = 'float')\n",
    "IBMData[newnumvars] = IBMData[newnumvars].apply(pd.to_numeric, axis = 1, downcast = 'float')\n",
    "IBMData[catvars] = IBMData[catvars].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now rename some of the categorical variable levels\n",
    "\n",
    "IBMData['BusinessTravel'].cat.rename_categories(\n",
    "        {'Travel_Rarely':'Rare','Travel_Frequently':'Freq','Non-Travel':'No'}, inplace = True)\n",
    "IBMData['Department'].cat.rename_categories(\n",
    "    {'Research & Development':'R&D','Sales':'Sales','Human Resources':'HR'}, inplace = True)\n",
    "IBMData['EducationField'].cat.rename_categories(\n",
    "    {'Life Sciences':'LifeSc','Technical Degree':'Technical','Human Resources':'EducHR'}, inplace = True)\n",
    "IBMData['JobRole'].cat.rename_categories(\n",
    "    {'Sales Executive':'SalexExec','Research Scientist':'Scientist','Laboratory Technician':'LabTech',\n",
    "    'Manufacturing Director':'ManufDir','Healthcare Representative':'HealthRep','Sales Representative':'SalesRep',\n",
    "    'Research Director':'ResDir','Human Resources':'RoleHR'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now re-categorize the Attrition (outcome) variable from object to int\n",
    "\n",
    "exit_status_map = {'Yes':1, 'No':0}\n",
    "IBMData['Attrition'] = IBMData['Attrition'].map(exit_status_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                          float32\n",
       "Attrition                      int64\n",
       "BusinessTravel              category\n",
       "DailyRate                    float32\n",
       "Department                  category\n",
       "DistanceFromHome             float32\n",
       "Education                    float32\n",
       "EducationField              category\n",
       "EnvironmentSatisfaction      float32\n",
       "Gender                      category\n",
       "HourlyRate                   float32\n",
       "JobInvolvement               float32\n",
       "JobLevel                     float32\n",
       "JobRole                     category\n",
       "JobSatisfaction              float32\n",
       "MaritalStatus               category\n",
       "MonthlyIncome                float32\n",
       "MonthlyRate                  float32\n",
       "NumCompaniesWorked           float32\n",
       "OverTime                    category\n",
       "PercentSalaryHike            float32\n",
       "PerformanceRating            float32\n",
       "RelationshipSatisfaction     float32\n",
       "StockOptionLevel             float32\n",
       "TotalWorkingYears            float32\n",
       "TrainingTimesLastYear        float32\n",
       "WorkLifeBalance              float32\n",
       "YearsAtCompany               float32\n",
       "YearsInCurrentRole           float32\n",
       "YearsSinceLastPromotion      float32\n",
       "YearsWithCurrManager         float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now check the data types of the columns\n",
    "\n",
    "IBMData.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate a new dataframe and then separate the predictors from outcome, and encode the dummies\n",
    "\n",
    "IBMDataNew = IBMData.copy(deep = True)\n",
    "IBMDataNewLabel = IBMDataNew['Attrition'].copy(deep = True)\n",
    "IBMDataNew.drop(['Attrition'], axis = 1, inplace = True)\n",
    "IBMDataNew = pd.get_dummies(IBMDataNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the initial data frame: (1470, 31)\n",
      "Size of the new data frame with predictors: (1470, 51)\n",
      "Size of the new data frame with labels: 1470\n",
      "\n",
      "Variable Names in initial data frame:  ['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n",
      "\n",
      "Variance Names in new data frame with predictors:  ['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'BusinessTravel_No', 'BusinessTravel_Freq', 'BusinessTravel_Rare', 'Department_HR', 'Department_R&D', 'Department_Sales', 'EducationField_EducHR', 'EducationField_LifeSc', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical', 'Gender_Female', 'Gender_Male', 'JobRole_HealthRep', 'JobRole_RoleHR', 'JobRole_LabTech', 'JobRole_Manager', 'JobRole_ManufDir', 'JobRole_ResDir', 'JobRole_Scientist', 'JobRole_SalexExec', 'JobRole_SalesRep', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'OverTime_No', 'OverTime_Yes']\n"
     ]
    }
   ],
   "source": [
    "# Now check some of the characteristics of the datasets created and the initial dataset\n",
    "\n",
    "print('Size of the initial data frame: {}'.format(IBMData.shape))\n",
    "print('Size of the new data frame with predictors: {}'.format(IBMDataNew.shape))\n",
    "print('Size of the new data frame with labels: {}'.format(len(IBMDataNewLabel)))\n",
    "print('\\nVariable Names in initial data frame: ', IBMData.columns.to_list())\n",
    "print('\\nVariance Names in new data frame with predictors: ', IBMDataNew.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of train and test predictors sets respectively are:  (1102, 51) (368, 51)\n",
      "Size of train and test labels respectively are:  1102 368\n",
      "\n",
      "Proportion of exits in train and test sets respectively:  0.161 0.163\n"
     ]
    }
   ],
   "source": [
    "# Now we split the data into training and test sets\n",
    "\n",
    "seed = 50\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(IBMDataNew, IBMDataNewLabel, test_size = 0.25, random_state = seed)\n",
    "\n",
    "# Now show some of the characteristics of the test and train sets\n",
    "print('Shapes of train and test predictors sets respectively are: ', Xtrain.shape, Xtest.shape)\n",
    "print('Size of train and test labels respectively are: ', len(Ytrain), len(Ytest))\n",
    "print('\\nProportion of exits in train and test sets respectively: ', round(sum(Ytrain)/len(Ytrain),3), round(sum(Ytest)/len(Ytest),3))\n",
    "\n",
    "# Now split the main data as well by using the index of the train and test sets\n",
    "IBMData_train = IBMData.loc[Xtrain.index,:].copy(deep = True)\n",
    "IBMData_test = IBMData.loc[Xtest.index,:].copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formula to be used for logistic regression:\n",
      "\n",
      "Attrition ~ Age + C(BusinessTravel, Treatment(reference = \"No\")) + DailyRate + C(Department, Treatment(reference = \"HR\")) + DistanceFromHome + Education + C(EducationField, Treatment(reference = \"EducHR\")) + EnvironmentSatisfaction + C(Gender, Treatment(reference = \"Female\")) + HourlyRate + JobInvolvement + JobLevel + C(JobRole, Treatment(reference = \"RoleHR\")) + JobSatisfaction + C(MaritalStatus, Treatment(reference = \"Single\")) + MonthlyIncome + MonthlyRate + NumCompaniesWorked + C(OverTime, Treatment(reference = \"No\")) + PercentSalaryHike + PerformanceRating + RelationshipSatisfaction + StockOptionLevel + TotalWorkingYears + TrainingTimesLastYear + WorkLifeBalance + YearsAtCompany + YearsInCurrentRole + YearsSinceLastPromotion + YearsWithCurrManager\n",
      "\n",
      "Attrition ~ Age + C(BusinessTravel) + DailyRate + C(Department) + DistanceFromHome + Education + C(EducationField) + EnvironmentSatisfaction + C(Gender) + HourlyRate + JobInvolvement + JobLevel + C(JobRole) + JobSatisfaction + C(MaritalStatus) + MonthlyIncome + MonthlyRate + NumCompaniesWorked + C(OverTime) + PercentSalaryHike + PerformanceRating + RelationshipSatisfaction + StockOptionLevel + TotalWorkingYears + TrainingTimesLastYear + WorkLifeBalance + YearsAtCompany + YearsInCurrentRole + YearsSinceLastPromotion + YearsWithCurrManager\n",
      "\n",
      "Attrition ~ Age + Attrition + BusinessTravel + DailyRate + Department + DistanceFromHome + Education + EducationField + EnvironmentSatisfaction + Gender + HourlyRate + JobInvolvement + JobLevel + JobRole + JobSatisfaction + MaritalStatus + MonthlyIncome + MonthlyRate + NumCompaniesWorked + OverTime + PercentSalaryHike + PerformanceRating + RelationshipSatisfaction + StockOptionLevel + TotalWorkingYears + TrainingTimesLastYear + WorkLifeBalance + YearsAtCompany + YearsInCurrentRole + YearsSinceLastPromotion + YearsWithCurrManager\n"
     ]
    }
   ],
   "source": [
    "## Defining the forumula to be used for the simple logistic regression\n",
    "# Iterate through the feature list in the main data\n",
    "# If the feature is int/float, then include that with a '+' sign in the formula string\n",
    "# If the feature is a category then include the correct base level in the category name\n",
    "\n",
    "catbasedict = {'BusinessTravel':'No', 'Department':'HR', 'EducationField':'EducHR','Gender':'Female',\n",
    "               'JobRole':'RoleHR', 'MaritalStatus':'Single','OverTime':'No'}\n",
    "counter = 0\n",
    "formula = 'Attrition ~ '\n",
    "for item in IBMData.columns.to_list():\n",
    "    if(item is not 'Attrition' and IBMData[item].dtype.name == 'float32'):\n",
    "        if(counter == 0):\n",
    "            formula = formula + item\n",
    "        else:\n",
    "            formula = formula + ' + ' + item\n",
    "        counter = counter + 1    \n",
    "    elif(item is not 'Attrition' and IBMData[item].dtype.name == 'category'):\n",
    "        if(counter == 0):\n",
    "            formula = formula + 'C(' + item + ', Treatment(reference = \"' + catbasedict[item] + '\"))'\n",
    "        else:\n",
    "            formula = formula + ' + ' + 'C(' + item + ', Treatment(reference = \"' + catbasedict[item] + '\"))'\n",
    "        counter = counter + 1\n",
    "\n",
    "counter = 0        \n",
    "formula_simple = 'Attrition ~ '\n",
    "for item in IBMData.columns.to_list():\n",
    "    if(item is not 'Attrition' and IBMData[item].dtype.name == 'float32'):\n",
    "        if(counter == 0):\n",
    "            formula_simple = formula_simple + item\n",
    "        else:\n",
    "            formula_simple = formula_simple + ' + ' + item\n",
    "        counter = counter + 1    \n",
    "    elif(item is not 'Attrition' and IBMData[item].dtype.name == 'category'):\n",
    "        if(counter == 0):\n",
    "            formula_simple = formula_simple + 'C(' + item + ')'\n",
    "        else:\n",
    "            formula_simple = formula_simple + ' + ' + 'C(' + item + ')'\n",
    "        counter = counter + 1\n",
    "        \n",
    "counter = 0\n",
    "formula_2verify = 'Attrition ~ '\n",
    "for item in IBMData.columns.to_list():\n",
    "    if(item is not 'Attrition' and counter == 0):\n",
    "        formula_2verify = formula_2verify + item\n",
    "    else:\n",
    "        formula_2verify = formula_2verify + ' + ' + item\n",
    "    counter = counter + 1    \n",
    "        \n",
    "print('Formula to be used for logistic regression:\\n')\n",
    "print(formula)\n",
    "print()\n",
    "print(formula_simple)\n",
    "print()\n",
    "print(formula_2verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.290094\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n"
     ]
    }
   ],
   "source": [
    "# Now Implement the logistic regression model \n",
    "\n",
    "logit_model = smf.logit(formula_simple, IBMData_train).fit(max_iter = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marginal Effects of the Explanatory Variables\n",
      "        Logit Marginal Effects       \n",
      "=====================================\n",
      "Dep. Variable:              Attrition\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "==================================================================================================\n",
      "                                    dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "C(BusinessTravel)[T.Freq]          0.1768      0.044      4.028      0.000       0.091       0.263\n",
      "C(BusinessTravel)[T.Rare]          0.1037      0.041      2.502      0.012       0.022       0.185\n",
      "C(Department)[T.R&D]               1.5368    547.740      0.003      0.998   -1072.015    1075.088\n",
      "C(Department)[T.Sales]             1.4777    547.740      0.003      0.998   -1072.074    1075.029\n",
      "C(EducationField)[T.LifeSc]       -0.0907      0.079     -1.153      0.249      -0.245       0.063\n",
      "C(EducationField)[T.Marketing]    -0.0674      0.084     -0.803      0.422      -0.232       0.097\n",
      "C(EducationField)[T.Medical]      -0.1064      0.079     -1.348      0.178      -0.261       0.048\n",
      "C(EducationField)[T.Other]        -0.1009      0.085     -1.189      0.234      -0.267       0.065\n",
      "C(EducationField)[T.Technical]     0.0024      0.080      0.030      0.976      -0.155       0.160\n",
      "C(Gender)[T.Male]                  0.0356      0.019      1.862      0.063      -0.002       0.073\n",
      "C(JobRole)[T.RoleHR]               1.6629    547.740      0.003      0.998   -1071.889    1075.214\n",
      "C(JobRole)[T.LabTech]              0.1494      0.050      2.958      0.003       0.050       0.248\n",
      "C(JobRole)[T.Manager]              0.0271      0.088      0.308      0.758      -0.145       0.199\n",
      "C(JobRole)[T.ManufDir]             0.0168      0.056      0.302      0.763      -0.092       0.126\n",
      "C(JobRole)[T.ResDir]              -0.0907      0.094     -0.966      0.334      -0.275       0.093\n",
      "C(JobRole)[T.Scientist]            0.0515      0.052      0.981      0.326      -0.051       0.154\n",
      "C(JobRole)[T.SalexExec]            0.1430      0.125      1.144      0.253      -0.102       0.388\n",
      "C(JobRole)[T.SalesRep]             0.2782      0.128      2.169      0.030       0.027       0.530\n",
      "C(MaritalStatus)[T.Married]        0.0477      0.028      1.712      0.087      -0.007       0.102\n",
      "C(MaritalStatus)[T.Single]         0.1294      0.036      3.636      0.000       0.060       0.199\n",
      "C(OverTime)[T.Yes]                 0.1700      0.018      9.626      0.000       0.135       0.205\n",
      "Age                               -0.0016      0.001     -1.171      0.242      -0.004       0.001\n",
      "DailyRate                      -5.354e-05   2.33e-05     -2.298      0.022   -9.92e-05   -7.88e-06\n",
      "DistanceFromHome                   0.0049      0.001      4.571      0.000       0.003       0.007\n",
      "Education                         -0.0061      0.009     -0.680      0.497      -0.024       0.011\n",
      "EnvironmentSatisfaction           -0.0376      0.008     -4.421      0.000      -0.054      -0.021\n",
      "HourlyRate                     -1.836e-05      0.000     -0.040      0.968      -0.001       0.001\n",
      "JobInvolvement                    -0.0464      0.012     -3.712      0.000      -0.071      -0.022\n",
      "JobLevel                          -0.0192      0.032     -0.606      0.544      -0.081       0.043\n",
      "JobSatisfaction                   -0.0381      0.008     -4.603      0.000      -0.054      -0.022\n",
      "MonthlyIncome                   7.276e-06   8.19e-06      0.889      0.374   -8.77e-06    2.33e-05\n",
      "MonthlyRate                     1.019e-08   1.31e-06      0.008      0.994   -2.56e-06    2.58e-06\n",
      "NumCompaniesWorked                 0.0165      0.004      4.211      0.000       0.009       0.024\n",
      "PercentSalaryHike                 -0.0051      0.004     -1.237      0.216      -0.013       0.003\n",
      "PerformanceRating                  0.0550      0.041      1.332      0.183      -0.026       0.136\n",
      "RelationshipSatisfaction          -0.0173      0.009     -2.033      0.042      -0.034      -0.001\n",
      "StockOptionLevel                  -0.0014      0.016     -0.088      0.930      -0.032       0.029\n",
      "TotalWorkingYears                 -0.0063      0.003     -2.119      0.034      -0.012      -0.000\n",
      "TrainingTimesLastYear             -0.0091      0.007     -1.218      0.223      -0.024       0.006\n",
      "WorkLifeBalance                   -0.0319      0.013     -2.527      0.011      -0.057      -0.007\n",
      "YearsAtCompany                     0.0048      0.004      1.206      0.228      -0.003       0.013\n",
      "YearsInCurrentRole                -0.0142      0.005     -2.978      0.003      -0.024      -0.005\n",
      "YearsSinceLastPromotion            0.0156      0.004      3.532      0.000       0.007       0.024\n",
      "YearsWithCurrManager              -0.0045      0.005     -0.866      0.386      -0.015       0.006\n",
      "==================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Getting the marginal effects of the explanatory variables\n",
    "\n",
    "print('Marginal Effects of the Explanatory Variables')\n",
    "AME = logit_model.get_margeff(at = 'overall', method = 'dydx')\n",
    "print(AME.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Estimates from Logit Model:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Attrition</td>    <th>  No. Observations:  </th>  <td>  1102</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1057</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    44</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 09 Mar 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.3417</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:32:38</td>     <th>  Log-Likelihood:    </th> <td> -319.68</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>False</td>      <th>  LL-Null:           </th> <td> -485.64</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>7.850e-46</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                   <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                      <td>  -16.7927</td> <td> 6162.894</td> <td>   -0.003</td> <td> 0.998</td> <td>-1.21e+04</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(BusinessTravel)[T.Freq]</th>      <td>    1.9898</td> <td>    0.504</td> <td>    3.950</td> <td> 0.000</td> <td>    1.002</td> <td>    2.977</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(BusinessTravel)[T.Rare]</th>      <td>    1.1666</td> <td>    0.469</td> <td>    2.485</td> <td> 0.013</td> <td>    0.246</td> <td>    2.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Department)[T.R&D]</th>           <td>   17.2909</td> <td> 6162.893</td> <td>    0.003</td> <td> 0.998</td> <td>-1.21e+04</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Department)[T.Sales]</th>         <td>   16.6258</td> <td> 6162.893</td> <td>    0.003</td> <td> 0.998</td> <td>-1.21e+04</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(EducationField)[T.LifeSc]</th>    <td>   -1.0206</td> <td>    0.888</td> <td>   -1.150</td> <td> 0.250</td> <td>   -2.760</td> <td>    0.719</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(EducationField)[T.Marketing]</th> <td>   -0.7587</td> <td>    0.946</td> <td>   -0.802</td> <td> 0.423</td> <td>   -2.613</td> <td>    1.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(EducationField)[T.Medical]</th>   <td>   -1.1969</td> <td>    0.891</td> <td>   -1.343</td> <td> 0.179</td> <td>   -2.944</td> <td>    0.550</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(EducationField)[T.Other]</th>     <td>   -1.1348</td> <td>    0.957</td> <td>   -1.185</td> <td> 0.236</td> <td>   -3.011</td> <td>    0.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(EducationField)[T.Technical]</th> <td>    0.0266</td> <td>    0.903</td> <td>    0.030</td> <td> 0.976</td> <td>   -1.742</td> <td>    1.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Gender)[T.Male]</th>              <td>    0.4004</td> <td>    0.216</td> <td>    1.852</td> <td> 0.064</td> <td>   -0.023</td> <td>    0.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(JobRole)[T.RoleHR]</th>           <td>   18.7100</td> <td> 6162.893</td> <td>    0.003</td> <td> 0.998</td> <td>-1.21e+04</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(JobRole)[T.LabTech]</th>          <td>    1.6809</td> <td>    0.575</td> <td>    2.921</td> <td> 0.003</td> <td>    0.553</td> <td>    2.809</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(JobRole)[T.Manager]</th>          <td>    0.3051</td> <td>    0.990</td> <td>    0.308</td> <td> 0.758</td> <td>   -1.634</td> <td>    2.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(JobRole)[T.ManufDir]</th>         <td>    0.1889</td> <td>    0.626</td> <td>    0.302</td> <td> 0.763</td> <td>   -1.039</td> <td>    1.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(JobRole)[T.ResDir]</th>           <td>   -1.0210</td> <td>    1.058</td> <td>   -0.965</td> <td> 0.334</td> <td>   -3.094</td> <td>    1.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(JobRole)[T.Scientist]</th>        <td>    0.5797</td> <td>    0.591</td> <td>    0.980</td> <td> 0.327</td> <td>   -0.580</td> <td>    1.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(JobRole)[T.SalexExec]</th>        <td>    1.6090</td> <td>    1.409</td> <td>    1.142</td> <td> 0.253</td> <td>   -1.153</td> <td>    4.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(JobRole)[T.SalesRep]</th>         <td>    3.1301</td> <td>    1.455</td> <td>    2.152</td> <td> 0.031</td> <td>    0.279</td> <td>    5.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(MaritalStatus)[T.Married]</th>    <td>    0.5370</td> <td>    0.315</td> <td>    1.705</td> <td> 0.088</td> <td>   -0.080</td> <td>    1.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(MaritalStatus)[T.Single]</th>     <td>    1.4556</td> <td>    0.409</td> <td>    3.559</td> <td> 0.000</td> <td>    0.654</td> <td>    2.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(OverTime)[T.Yes]</th>             <td>    1.9128</td> <td>    0.225</td> <td>    8.512</td> <td> 0.000</td> <td>    1.472</td> <td>    2.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Age</th>                            <td>   -0.0185</td> <td>    0.016</td> <td>   -1.168</td> <td> 0.243</td> <td>   -0.049</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DailyRate</th>                      <td>   -0.0006</td> <td>    0.000</td> <td>   -2.282</td> <td> 0.023</td> <td>   -0.001</td> <td>-8.49e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DistanceFromHome</th>               <td>    0.0548</td> <td>    0.012</td> <td>    4.443</td> <td> 0.000</td> <td>    0.031</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Education</th>                      <td>   -0.0681</td> <td>    0.100</td> <td>   -0.679</td> <td> 0.497</td> <td>   -0.265</td> <td>    0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>EnvironmentSatisfaction</th>        <td>   -0.4226</td> <td>    0.098</td> <td>   -4.297</td> <td> 0.000</td> <td>   -0.615</td> <td>   -0.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HourlyRate</th>                     <td>   -0.0002</td> <td>    0.005</td> <td>   -0.040</td> <td> 0.968</td> <td>   -0.010</td> <td>    0.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JobInvolvement</th>                 <td>   -0.5216</td> <td>    0.144</td> <td>   -3.626</td> <td> 0.000</td> <td>   -0.804</td> <td>   -0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JobLevel</th>                       <td>   -0.2165</td> <td>    0.357</td> <td>   -0.606</td> <td> 0.545</td> <td>   -0.917</td> <td>    0.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JobSatisfaction</th>                <td>   -0.4287</td> <td>    0.096</td> <td>   -4.462</td> <td> 0.000</td> <td>   -0.617</td> <td>   -0.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MonthlyIncome</th>                  <td> 8.187e-05</td> <td> 9.23e-05</td> <td>    0.887</td> <td> 0.375</td> <td> -9.9e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MonthlyRate</th>                    <td> 1.146e-07</td> <td> 1.47e-05</td> <td>    0.008</td> <td> 0.994</td> <td>-2.88e-05</td> <td>  2.9e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NumCompaniesWorked</th>             <td>    0.1861</td> <td>    0.045</td> <td>    4.100</td> <td> 0.000</td> <td>    0.097</td> <td>    0.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PercentSalaryHike</th>              <td>   -0.0573</td> <td>    0.046</td> <td>   -1.236</td> <td> 0.217</td> <td>   -0.148</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PerformanceRating</th>              <td>    0.6193</td> <td>    0.466</td> <td>    1.329</td> <td> 0.184</td> <td>   -0.294</td> <td>    1.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RelationshipSatisfaction</th>       <td>   -0.1949</td> <td>    0.097</td> <td>   -2.018</td> <td> 0.044</td> <td>   -0.384</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>StockOptionLevel</th>               <td>   -0.0155</td> <td>    0.175</td> <td>   -0.088</td> <td> 0.930</td> <td>   -0.359</td> <td>    0.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TotalWorkingYears</th>              <td>   -0.0713</td> <td>    0.034</td> <td>   -2.104</td> <td> 0.035</td> <td>   -0.138</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TrainingTimesLastYear</th>          <td>   -0.1022</td> <td>    0.084</td> <td>   -1.215</td> <td> 0.224</td> <td>   -0.267</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>WorkLifeBalance</th>                <td>   -0.3591</td> <td>    0.143</td> <td>   -2.507</td> <td> 0.012</td> <td>   -0.640</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>YearsAtCompany</th>                 <td>    0.0540</td> <td>    0.045</td> <td>    1.204</td> <td> 0.228</td> <td>   -0.034</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>YearsInCurrentRole</th>             <td>   -0.1598</td> <td>    0.054</td> <td>   -2.947</td> <td> 0.003</td> <td>   -0.266</td> <td>   -0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>YearsSinceLastPromotion</th>        <td>    0.1760</td> <td>    0.051</td> <td>    3.478</td> <td> 0.001</td> <td>    0.077</td> <td>    0.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>YearsWithCurrManager</th>           <td>   -0.0505</td> <td>    0.058</td> <td>   -0.865</td> <td> 0.387</td> <td>   -0.165</td> <td>    0.064</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              Attrition   No. Observations:                 1102\n",
       "Model:                          Logit   Df Residuals:                     1057\n",
       "Method:                           MLE   Df Model:                           44\n",
       "Date:                Tue, 09 Mar 2021   Pseudo R-squ.:                  0.3417\n",
       "Time:                        12:32:38   Log-Likelihood:                -319.68\n",
       "converged:                      False   LL-Null:                       -485.64\n",
       "Covariance Type:            nonrobust   LLR p-value:                 7.850e-46\n",
       "==================================================================================================\n",
       "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------\n",
       "Intercept                        -16.7927   6162.894     -0.003      0.998   -1.21e+04    1.21e+04\n",
       "C(BusinessTravel)[T.Freq]          1.9898      0.504      3.950      0.000       1.002       2.977\n",
       "C(BusinessTravel)[T.Rare]          1.1666      0.469      2.485      0.013       0.246       2.087\n",
       "C(Department)[T.R&D]              17.2909   6162.893      0.003      0.998   -1.21e+04    1.21e+04\n",
       "C(Department)[T.Sales]            16.6258   6162.893      0.003      0.998   -1.21e+04    1.21e+04\n",
       "C(EducationField)[T.LifeSc]       -1.0206      0.888     -1.150      0.250      -2.760       0.719\n",
       "C(EducationField)[T.Marketing]    -0.7587      0.946     -0.802      0.423      -2.613       1.095\n",
       "C(EducationField)[T.Medical]      -1.1969      0.891     -1.343      0.179      -2.944       0.550\n",
       "C(EducationField)[T.Other]        -1.1348      0.957     -1.185      0.236      -3.011       0.742\n",
       "C(EducationField)[T.Technical]     0.0266      0.903      0.030      0.976      -1.742       1.796\n",
       "C(Gender)[T.Male]                  0.4004      0.216      1.852      0.064      -0.023       0.824\n",
       "C(JobRole)[T.RoleHR]              18.7100   6162.893      0.003      0.998   -1.21e+04    1.21e+04\n",
       "C(JobRole)[T.LabTech]              1.6809      0.575      2.921      0.003       0.553       2.809\n",
       "C(JobRole)[T.Manager]              0.3051      0.990      0.308      0.758      -1.634       2.245\n",
       "C(JobRole)[T.ManufDir]             0.1889      0.626      0.302      0.763      -1.039       1.417\n",
       "C(JobRole)[T.ResDir]              -1.0210      1.058     -0.965      0.334      -3.094       1.052\n",
       "C(JobRole)[T.Scientist]            0.5797      0.591      0.980      0.327      -0.580       1.739\n",
       "C(JobRole)[T.SalexExec]            1.6090      1.409      1.142      0.253      -1.153       4.371\n",
       "C(JobRole)[T.SalesRep]             3.1301      1.455      2.152      0.031       0.279       5.981\n",
       "C(MaritalStatus)[T.Married]        0.5370      0.315      1.705      0.088      -0.080       1.154\n",
       "C(MaritalStatus)[T.Single]         1.4556      0.409      3.559      0.000       0.654       2.257\n",
       "C(OverTime)[T.Yes]                 1.9128      0.225      8.512      0.000       1.472       2.353\n",
       "Age                               -0.0185      0.016     -1.168      0.243      -0.049       0.013\n",
       "DailyRate                         -0.0006      0.000     -2.282      0.023      -0.001   -8.49e-05\n",
       "DistanceFromHome                   0.0548      0.012      4.443      0.000       0.031       0.079\n",
       "Education                         -0.0681      0.100     -0.679      0.497      -0.265       0.128\n",
       "EnvironmentSatisfaction           -0.4226      0.098     -4.297      0.000      -0.615      -0.230\n",
       "HourlyRate                        -0.0002      0.005     -0.040      0.968      -0.010       0.010\n",
       "JobInvolvement                    -0.5216      0.144     -3.626      0.000      -0.804      -0.240\n",
       "JobLevel                          -0.2165      0.357     -0.606      0.545      -0.917       0.484\n",
       "JobSatisfaction                   -0.4287      0.096     -4.462      0.000      -0.617      -0.240\n",
       "MonthlyIncome                   8.187e-05   9.23e-05      0.887      0.375    -9.9e-05       0.000\n",
       "MonthlyRate                     1.146e-07   1.47e-05      0.008      0.994   -2.88e-05     2.9e-05\n",
       "NumCompaniesWorked                 0.1861      0.045      4.100      0.000       0.097       0.275\n",
       "PercentSalaryHike                 -0.0573      0.046     -1.236      0.217      -0.148       0.034\n",
       "PerformanceRating                  0.6193      0.466      1.329      0.184      -0.294       1.532\n",
       "RelationshipSatisfaction          -0.1949      0.097     -2.018      0.044      -0.384      -0.006\n",
       "StockOptionLevel                  -0.0155      0.175     -0.088      0.930      -0.359       0.328\n",
       "TotalWorkingYears                 -0.0713      0.034     -2.104      0.035      -0.138      -0.005\n",
       "TrainingTimesLastYear             -0.1022      0.084     -1.215      0.224      -0.267       0.063\n",
       "WorkLifeBalance                   -0.3591      0.143     -2.507      0.012      -0.640      -0.078\n",
       "YearsAtCompany                     0.0540      0.045      1.204      0.228      -0.034       0.142\n",
       "YearsInCurrentRole                -0.1598      0.054     -2.947      0.003      -0.266      -0.054\n",
       "YearsSinceLastPromotion            0.1760      0.051      3.478      0.001       0.077       0.275\n",
       "YearsWithCurrManager              -0.0505      0.058     -0.865      0.387      -0.165       0.064\n",
       "==================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Baseline Estimates from Logit Model:\\n')\n",
    "logit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model implemented above clearly does not converge. This is possibly because of collinearity in the\n",
    "features being used or the large number of categorical variables being used. To get around this problem \n",
    "we will use the Recursive Feature Elimination (possibly with Cross-Validation) method to select among the \n",
    "set of all features, and then run a logistic regression on the final list of features selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following algorithm for RFE\n",
    "# First one hot encode the categorical variables and then use logistic regression along with RFE to select features\n",
    "# Then even if one of the levels of any categorical variable is included, then select all of the levels for that categorical variable\n",
    "# In RFE we need to select the number of variables to keep, so iterate over the number of variables (including one hot encoded dummies)\n",
    "# Then show the abs(log likelihood value)  \n",
    "\n",
    "def RFE_Logit(Xtrain, Ytrain, include):\n",
    "    \n",
    "    models = []\n",
    "    selected_features = []\n",
    "    \n",
    "    for i in np.arange(1, Xtrain.shape[1]+1):\n",
    "        logreg = LogisticRegression()\n",
    "        rfe = RFE(estimator = logreg, n_features_to_select = i)\n",
    "        rfe = rfe.fit(Xtrain, Ytrain)\n",
    "        cols2select = Xtrain.columns[rfe.support_].to_list()\n",
    "    \n",
    "        if include:\n",
    "            print('Yay')\n",
    "        \n",
    "   \n",
    "        logit_model = sm.Logit(Ytrain, Xtrain[cols2select]).fit(disp = 0)\n",
    "    \n",
    "        models.append(logit_model)\n",
    "        selected_features.append(cols2select)\n",
    "        \n",
    "    return models, selected_features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "C:\\Users\\Public\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n"
     ]
    }
   ],
   "source": [
    "# Now training the model with no extra feature inclusion\n",
    "\n",
    "models, selected_features = RFE_Logit(Xtrain, Ytrain, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now plot the log likelhood value against the number of features selected and then choose the one with the highest\n",
    "\n",
    "num_features = np.arange(1, Xtrain.shape[1]+1)\n",
    "loglike = [logitmodel.llf for logitmodel in models]\n",
    "convergence = [logitmodel.mle_retvals['converged'] for logitmodel in models]\n",
    "newframe = pd.DataFrame({'num_features':num_features, 'loglike':loglike, 'convergence':convergence})\n",
    "label2color = {True:'green', False:'red'}\n",
    "newframe['color'] = newframe['convergence'].apply(lambda convergence: label2color[convergence])\n",
    "\n",
    "# Now iterate through the column 'convergence' and then select the breakpoints where convergence changes\n",
    "breakpoints = []\n",
    "start = 0\n",
    "for i in np.arange(len(newframe)-1):\n",
    "    if(newframe.iloc[i,2]!=newframe.iloc[i+1,2]):\n",
    "        breakpoints.append((start, i, newframe.iloc[i,3]))\n",
    "        start = i + 1\n",
    "breakpoints.append((start, i+1, newframe.iloc[start,3]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for (start, end, color) in breakpoints:\n",
    "    if start > 0:\n",
    "        start = start - 1\n",
    "    ax.plot(newframe.num_features[start:end+1], newframe.loglike[start:end+1], color = color, label = '')\n",
    "ax.set_title('Log-Likelihood Value v/s number of features\\n [Green Indicates convergence]')\n",
    "ax.set_xlabel('Number of features')\n",
    "ax.set_ylabel('Log-Likelihood')\n",
    "plt.show()        \n",
    "\n",
    "# # Now select the model with highest log-likelihood value along with convergence\n",
    "# optimalindex = heapq.nlargest(1, range(len(newframe)), key = lambda x: newframe.loglike[x])\n",
    "# print(optimalindex)\n",
    "\n",
    "# newframe = newframe.query('convergence == True')\n",
    "# print(newframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issues to take care of: regularization in LogisticRegression, regularization in LogisticRegression, regularization parameter tuning, \n",
    "# Parameter selection whether to include all the remaining levels, other hyperparameter tuning, choosing the statistics to plot\n",
    "\n",
    "# First do feature selection and show metrics\n",
    "# Then with selected features do logistic regression with hyperparaneter tuning \n",
    "# Then do a combination of feature selection and hyperparamerer tuning \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
